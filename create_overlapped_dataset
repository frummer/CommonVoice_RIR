from datasets import load_dataset
import soundfile as sf
import librosa
import numpy as np
import scipy.signal
from uuid import uuid4
import random
import os
import json
TARGET_SAMPLE_RATE = 8000
DESIRED_MIXTURES_AMOUNT = 5

def load_random_rir(directory):
    """Choose a random RIR from a directory."""
    rir_files = [f for f in os.listdir(directory) if f.endswith('.wav')]
    random_rir_file = random.choice(rir_files)
    rir_path = os.path.join(directory, random_rir_file)
    rir_waveform, rir_sample_rate = librosa.load(rir_path, sr=None)
    if rir_sample_rate != TARGET_SAMPLE_RATE:
        rir_waveform = librosa.resample(rir_waveform, orig_sr=rir_sample_rate, target_sr=TARGET_SAMPLE_RATE)
    return rir_waveform

def apply_rir_to_audio(audio, rir):
    """Convolve the audio with the RIR."""
    convolved_audio = scipy.signal.convolve(audio, rir, mode='full')
    return convolved_audio


def mix_audio(file1_path, file2_path, transcription1, transcription2, output_path, metadata):
    # Load audio files
    y1, sr1 = librosa.load(file1_path, sr=None)
    y2, sr2 = librosa.load(file2_path, sr=None)

    if sr1 != TARGET_SAMPLE_RATE:
        y1= librosa.resample(y1, orig_sr=sr1, target_sr=TARGET_SAMPLE_RATE)

    if sr2 != TARGET_SAMPLE_RATE:
        y2= librosa.resample(y2, orig_sr=sr2, target_sr=TARGET_SAMPLE_RATE)

    # Calculate original lengths
    length1 = len(y1) / TARGET_SAMPLE_RATE
    length2 = len(y2) / TARGET_SAMPLE_RATE

    # Pad the shorter file
    if len(y1) > len(y2):
        pad_length = (len(y1) - len(y2)) / TARGET_SAMPLE_RATE
        y2 = np.pad(y2, (0, len(y1) - len(y2)), mode='constant')
        padding1, padding2 = 0, pad_length
    else:
        pad_length = (len(y2) - len(y1)) / TARGET_SAMPLE_RATE
        y1 = np.pad(y1, (0, len(y2) - len(y1)), mode='constant')
        padding1, padding2 = pad_length, 0

    # Mix the audio
    mixed_audio = y1 + y2
    mixed_audio = mixed_audio / np.max(np.abs(mixed_audio))

    # Generate unique ID and save mixed audio
    unique_id = str(uuid4())
    subdirectory_path = os.path.join(output_path, unique_id)
    os.makedirs(subdirectory_path, exist_ok=True)
    mixed_audio_file = os.path.join(subdirectory_path, f"{unique_id}.wav")
    sf.write(mixed_audio_file, mixed_audio, TARGET_SAMPLE_RATE)

    # Save original files
    original_file1 = os.path.join(subdirectory_path, os.path.basename(file1_path))
    original_file2 = os.path.join(subdirectory_path, os.path.basename(file2_path))
    sf.write(original_file1,y1,TARGET_SAMPLE_RATE)
    sf.write(original_file2,y2,TARGET_SAMPLE_RATE)

    # apply Room impulse on audio and save file
    rir = load_random_rir(rir_directory)
    ff_audio = apply_rir_to_audio(mixed_audio, rir)
    mixed_audio_ff_file = os.path.join(subdirectory_path, f"ff_{unique_id}.wav")
    sf.write(mixed_audio_ff_file, ff_audio, TARGET_SAMPLE_RATE)

    # Save metadata
    metadata_entry = {
        "id": unique_id,
        "length_seconds": len(mixed_audio) / TARGET_SAMPLE_RATE,
        "original_files": [
            {
                "file": os.path.basename(file1_path),
                "original_length": length1,
                "padding_seconds": padding1,
                "transcription": transcription1
            },
            {
                "file": os.path.basename(file2_path),
                "original_length": length2,
                "padding_seconds": padding2,
                "transcription": transcription2
            }
        ]
    }
    metadata.append(metadata_entry)

def process_common_voice(dataset, output_dir, metadata_file):
    os.makedirs(output_dir, exist_ok=True)
    metadata = []

    # Extract audio paths and transcriptions
    audio_files = dataset["path"]
    transcriptions = dataset["sentence"]

    # Shuffle the audio files and their corresponding transcriptions
    data = list(zip(audio_files, transcriptions))
    random.shuffle(data)

    # Create overlapping pairs (non-redundant)
    for i in range(0, DESIRED_MIXTURES_AMOUNT*2 - 1, 2):  # Step by 2 to ensure no file is reused
        file1_path, transcription1 = data[i]
        file2_path, transcription2 = data[i + 1]
        mix_audio(file1_path, file2_path, transcription1, transcription2, output_dir, metadata)

    # Save metadata as JSON
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=4, ensure_ascii=False)

if __name__ == "__main__":
    dataset = load_dataset("mozilla-foundation/common_voice_12_0", "ar", split="test",trust_remote_code=True)

    # Directories
    output_directory = "./overlapped_samples"
    metadata_file_path = "./overlapped_samples/metadata.json"
    rir_directory = './MIT_RIR'  # Path to  RIR directory

    # Process dataset
    process_common_voice(dataset=dataset, output_dir=output_directory, metadata_file=metadata_file_path)